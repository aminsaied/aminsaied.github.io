"use strict";(self.webpackChunkaminsaied=self.webpackChunkaminsaied||[]).push([[1477],{10:t=>{t.exports=JSON.parse('{"blogPosts":[{"id":"/2021/09/24/roberta-electra","metadata":{"permalink":"/blog/2021/09/24/roberta-electra","source":"@site/blog/2021-09-24-roberta-electra/index.md","title":"\u2728Talk: Roberta and Electra","description":"Abstract. In this talk I give an overview of two important transformer","date":"2021-09-24T00:00:00.000Z","formattedDate":"September 24, 2021","tags":[{"label":"talk","permalink":"/blog/tags/talk"},{"label":"nlp","permalink":"/blog/tags/nlp"}],"readingTime":0.135,"truncated":true,"authors":[{"name":"Amin Saied","title":"ML Engineer, Microsoft","url":"https://github.com/aminsaied","imageURL":"https://github.com/aminsaied.png","key":"aminsaied"}],"frontMatter":{"title":"\u2728Talk: Roberta and Electra","authors":["aminsaied"],"tags":["talk","nlp"]},"nextItem":{"title":"\u2728Talk: Graph Neural Networks","permalink":"/blog/2021/08/19/gnns"}},"content":"**Abstract.** In this talk I give an overview of two important transformer\\nmodels in NLP.\\n\\n\x3c!--truncate--\x3e\\n\\n\ud83d\ude34 Lazy blog - just a link to the talk\'s [\ud83d\udccbPDF](2021-09-24-deck.pdf)."},{"id":"/2021/08/19/gnns","metadata":{"permalink":"/blog/2021/08/19/gnns","source":"@site/blog/2021-08-19-gnns/index.md","title":"\u2728Talk: Graph Neural Networks","description":"Abstract. A gentle introduction to graph neural networks (GNN)s","date":"2021-08-19T00:00:00.000Z","formattedDate":"August 19, 2021","tags":[{"label":"talk","permalink":"/blog/tags/talk"},{"label":"gnn","permalink":"/blog/tags/gnn"},{"label":"intro","permalink":"/blog/tags/intro"}],"readingTime":0.255,"truncated":true,"authors":[{"name":"Amin Saied","title":"ML Engineer, Microsoft","url":"https://github.com/aminsaied","imageURL":"https://github.com/aminsaied.png","key":"aminsaied"}],"frontMatter":{"title":"\u2728Talk: Graph Neural Networks","authors":["aminsaied"],"tags":["talk","gnn","intro"]},"prevItem":{"title":"\u2728Talk: Roberta and Electra","permalink":"/blog/2021/09/24/roberta-electra"},"nextItem":{"title":"\u2728Talk: Introduction to PyTorch Lightning","permalink":"/blog/2021/05/07/pytorch-lightning"}},"content":"**Abstract.** A gentle introduction to graph neural networks (GNN)s\\n\\n- Attention mechanisms in GNNs\\n- PyTorch Geometric example: Karate Dataset\\n- This will be an introductory talk with lots of pictures (in fact, it \\n  will almost exclusively be pictures!)\\n\\n\x3c!--truncate--\x3e\\n\\n\ud83d\ude34 Lazy blog - just a link to the talk\'s [\ud83d\udccbPDF](2021-08-19-deck.pdf)."},{"id":"/2021/05/07/pytorch-lightning","metadata":{"permalink":"/blog/2021/05/07/pytorch-lightning","source":"@site/blog/2021-05-07-pytorch-lightning/index.md","title":"\u2728Talk: Introduction to PyTorch Lightning","description":"Abstract. A hands on guide to using PyTorch Lightning. Concretely,","date":"2021-05-07T00:00:00.000Z","formattedDate":"May 7, 2021","tags":[{"label":"talk","permalink":"/blog/tags/talk"},{"label":"nlp","permalink":"/blog/tags/nlp"}],"readingTime":0.21,"truncated":true,"authors":[{"name":"Amin Saied","title":"ML Engineer, Microsoft","url":"https://github.com/aminsaied","imageURL":"https://github.com/aminsaied.png","key":"aminsaied"}],"frontMatter":{"title":"\u2728Talk: Introduction to PyTorch Lightning","authors":["aminsaied"],"tags":["talk","nlp"]},"prevItem":{"title":"\u2728Talk: Graph Neural Networks","permalink":"/blog/2021/08/19/gnns"},"nextItem":{"title":"\u2728Talk: Pipeline Model Parallelism","permalink":"/blog/2021/02/12/parallelism"}},"content":"**Abstract.** A hands on guide to using PyTorch Lightning. Concretely,\\nwe grab the GPT-2 model from Huggingface and build a lightning module\\nto train it (more or less from scratch).\\n\\n\x3c!--truncate--\x3e\\n\\n\ud83d\ude34 Lazy blog - just a link to the talk\'s [\ud83d\udccbPDF](2021-05-07-deck.pdf)."},{"id":"/2021/02/12/parallelism","metadata":{"permalink":"/blog/2021/02/12/parallelism","source":"@site/blog/2021-02-12-parallelism/index.md","title":"\u2728Talk: Pipeline Model Parallelism","description":"Abstract. Models are getting increasingly large, to the point that","date":"2021-02-12T00:00:00.000Z","formattedDate":"February 12, 2021","tags":[{"label":"talk","permalink":"/blog/tags/talk"},{"label":"nlp","permalink":"/blog/tags/nlp"}],"readingTime":0.235,"truncated":true,"authors":[{"name":"Amin Saied","title":"ML Engineer, Microsoft","url":"https://github.com/aminsaied","imageURL":"https://github.com/aminsaied.png","key":"aminsaied"}],"frontMatter":{"title":"\u2728Talk: Pipeline Model Parallelism","authors":["aminsaied"],"tags":["talk","nlp"]},"prevItem":{"title":"\u2728Talk: Introduction to PyTorch Lightning","permalink":"/blog/2021/05/07/pytorch-lightning"},"nextItem":{"title":"\u2728Talk: GPT-1 and GPT-2 Review","permalink":"/blog/2021/01/15/gpt1-gpt2"}},"content":"**Abstract.** Models are getting increasingly large, to the point that\\nthey don\'t always fit on a single device! We discuss some techniques to\\npartition models over multiple devices, from plain PyTorch to libraries\\nlike deepspeed.\\n\\n\x3c!--truncate--\x3e\\n\\n\ud83d\ude34 Lazy blog - just a link to the talk\'s [\ud83d\udccbPDF](2021-02-12-deck.pdf)."},{"id":"/2021/01/15/gpt1-gpt2","metadata":{"permalink":"/blog/2021/01/15/gpt1-gpt2","source":"@site/blog/2021-01-15-gpt1-gpt2/index.md","title":"\u2728Talk: GPT-1 and GPT-2 Review","description":"Abstract. In this talk we\'ll review the GPT-1 paper","date":"2021-01-15T00:00:00.000Z","formattedDate":"January 15, 2021","tags":[{"label":"talk","permalink":"/blog/tags/talk"},{"label":"nlp","permalink":"/blog/tags/nlp"}],"readingTime":0.365,"truncated":true,"authors":[{"name":"Amin Saied","title":"ML Engineer, Microsoft","url":"https://github.com/aminsaied","imageURL":"https://github.com/aminsaied.png","key":"aminsaied"}],"frontMatter":{"title":"\u2728Talk: GPT-1 and GPT-2 Review","authors":["aminsaied"],"tags":["talk","nlp"]},"prevItem":{"title":"\u2728Talk: Pipeline Model Parallelism","permalink":"/blog/2021/02/12/parallelism"},"nextItem":{"title":"\u2728Talk: Introduction to \ud83e\udd17Hugging Face","permalink":"/blog/2020/12/18/intro-to-huggingface"}},"content":"**Abstract.** In this talk we\'ll review the GPT-1 paper\\n[Improving Language Understanding by Generative Pre-Training - Radford et al][1].\\nBy way of setting the stage we will give a brief review of the\\nTransformer architecture.\\n\\nNote: We didn\'t have time to cover GPT-2 in this talk, but some slides on the topic made it into the deck.\\n\\n\x3c!--truncate--\x3e\\n\\n\ud83d\ude34 Lazy blog - just a link to the talk\'s [\ud83d\udccbPDF](2021-01-15-deck.pdf).\\n\\n[1]: https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf\\n[2]: https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf"},{"id":"/2020/12/18/intro-to-huggingface","metadata":{"permalink":"/blog/2020/12/18/intro-to-huggingface","source":"@site/blog/2020-12-18-intro-to-huggingface/index.md","title":"\u2728Talk: Introduction to \ud83e\udd17Hugging Face","description":"Abstract. In this talk we\'ll review the \ud83e\udd17Hugging Face Transformers","date":"2020-12-18T00:00:00.000Z","formattedDate":"December 18, 2020","tags":[{"label":"talk","permalink":"/blog/tags/talk"},{"label":"nlp","permalink":"/blog/tags/nlp"}],"readingTime":0.33,"truncated":true,"authors":[{"name":"Amin Saied","title":"ML Engineer, Microsoft","url":"https://github.com/aminsaied","imageURL":"https://github.com/aminsaied.png","key":"aminsaied"}],"frontMatter":{"title":"\u2728Talk: Introduction to \ud83e\udd17Hugging Face","authors":["aminsaied"],"tags":["talk","nlp"]},"prevItem":{"title":"\u2728Talk: GPT-1 and GPT-2 Review","permalink":"/blog/2021/01/15/gpt1-gpt2"}},"content":"**Abstract.** In this talk we\'ll review the \ud83e\udd17Hugging Face Transformers\\nlibrary. This is an open-source library with the stated goal to \\"democratize\\nNLP\\". We\'ll briefly review some background, explain what problems Huggingface\\nis trying to address, and cover some of the tools and techniques they provide.\\nWe will not assume any familiarity with transformers.\\n\\n\x3c!--truncate--\x3e\\n\\n\ud83d\ude34 Lazy blog - just a link to the talk\'s [\ud83d\udccbPDF](2020-12-18-deck.pdf)."}]}')}}]);